"""Quick script to index sample documents into Pinecone.

Run this to populate your Pinecone index with documents before asking questions.
"""

import os
from pathlib import Path

from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pinecone import Pinecone

# Load environment variables
load_dotenv()

PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


def index_sample_text():
    """Index sample text documents about vector databases."""
    
    print("üîß Initializing embeddings and vector store...")
    
    # Initialize embeddings
    embeddings = OpenAIEmbeddings(
        api_key=OPENAI_API_KEY,
        model="text-embedding-3-large"
    )
    
    # Initialize Pinecone
    pc = Pinecone(api_key=PINECONE_API_KEY)
    index = pc.Index(PINECONE_INDEX_NAME)
    
    # Check if index has vectors
    stats = index.describe_index_stats()
    print(f"üìä Current vector count: {stats['total_vector_count']}")
    
    # Sample documents about vector databases
    sample_docs = [
        {
            "content": """HNSW (Hierarchical Navigable Small World) is a graph-based indexing algorithm 
            for approximate nearest neighbor search in high-dimensional spaces. It provides fast search 
            times by organizing data points in a hierarchical graph structure. The algorithm builds 
            multiple layers, where higher layers have fewer nodes and enable quick navigation to the 
            target region. HNSW achieves excellent performance with sub-linear search complexity.""",
            "metadata": {"page": 5, "source": "vector_db_paper.pdf"}
        },
        {
            "content": """The hierarchical structure of HNSW graphs allows for efficient navigation 
            through the vector space. Each layer acts as a skip list, enabling the algorithm to quickly 
            jump to relevant regions before performing detailed search in the bottom layer. This 
            multi-scale approach is key to HNSW's performance advantage over flat indexes.""",
            "metadata": {"page": 6, "source": "vector_db_paper.pdf"}
        },
        {
            "content": """Locality-Sensitive Hashing (LSH) is a technique for approximate nearest neighbor 
            search that uses hash functions to map similar vectors to the same buckets. LSH reduces the 
            search space by focusing only on vectors that hash to the same or nearby buckets as the query 
            vector. This probabilistic approach trades some accuracy for significant speed improvements.""",
            "metadata": {"page": 7, "source": "vector_db_paper.pdf"}
        },
        {
            "content": """Inverted File (IVF) indexing partitions the vector space into Voronoi cells 
            using k-means clustering. During search, only vectors in the nearest clusters are examined, 
            dramatically reducing the number of comparisons needed. IVF can be combined with product 
            quantization for additional compression and speed improvements.""",
            "metadata": {"page": 8, "source": "vector_db_paper.pdf"}
        },
        {
            "content": """Product Quantization (PQ) is a compression technique that divides vectors into 
            subvectors and quantizes each subvector independently using a learned codebook. This approach 
            achieves significant memory savings while enabling fast approximate distance computations. 
            PQ is often used in conjunction with other indexing methods like IVF.""",
            "metadata": {"page": 9, "source": "vector_db_paper.pdf"}
        },
        {
            "content": """Vector databases are optimized for storing and querying high-dimensional 
            embeddings generated by machine learning models. They provide specialized indexing structures 
            and query capabilities designed for similarity search operations. Common use cases include 
            semantic search, recommendation systems, and retrieval-augmented generation (RAG) systems.""",
            "metadata": {"page": 3, "source": "vector_db_paper.pdf"}
        },
        {
            "content": """Embeddings are dense vector representations of data that capture semantic 
            meaning. Modern embedding models like OpenAI's text-embedding-3-large can represent text 
            in thousands of dimensions, where similar concepts are positioned close together in the 
            vector space. This enables semantic search that goes beyond keyword matching.""",
            "metadata": {"page": 4, "source": "vector_db_paper.pdf"}
        },
        {
            "content": """Approximate Nearest Neighbor (ANN) search is essential for scaling vector 
            databases to millions or billions of vectors. Exact search becomes impractical at scale, 
            so ANN algorithms trade small amounts of accuracy for orders of magnitude speed improvements. 
            Techniques like HNSW, LSH, and IVF enable sub-millisecond queries on large datasets.""",
            "metadata": {"page": 10, "source": "vector_db_paper.pdf"}
        },
    ]
    
    print(f"üìù Indexing {len(sample_docs)} sample documents...")
    
    # Convert to LangChain Document format
    from langchain_core.documents import Document
    documents = [
        Document(page_content=doc["content"], metadata=doc["metadata"])
        for doc in sample_docs
    ]
    
    # Create vector store and add documents
    vector_store = PineconeVectorStore(
        index=index,
        embedding=embeddings,
        text_key="text"
    )
    
    # Add documents
    ids = vector_store.add_documents(documents)
    print(f"‚úÖ Successfully indexed {len(ids)} documents!")
    
    # Verify
    stats = index.describe_index_stats()
    print(f"üìä New vector count: {stats['total_vector_count']}")
    
    print("\nüéâ Indexing complete! You can now ask questions in the frontend.")
    print("   Example: 'What is HNSW indexing?'")


def index_pdf_files():
    """Index PDF files from the data/ directory."""
    
    data_dir = Path("data")
    
    if not data_dir.exists():
        print(f"‚ùå Directory not found: {data_dir}")
        print("   Creating data/ directory...")
        data_dir.mkdir(exist_ok=True)
        print("   Please place PDF files in the data/ directory and run again.")
        return
    
    pdf_files = list(data_dir.glob("*.pdf"))
    
    if not pdf_files:
        print("‚ùå No PDF files found in data/ directory")
        print("   Please add PDF files and run again.")
        print("   Or use 'python index_documents.py --sample' to index sample text.")
        return
    
    print(f"üìÅ Found {len(pdf_files)} PDF files")
    
    # Initialize components
    embeddings = OpenAIEmbeddings(
        api_key=OPENAI_API_KEY,
        model="text-embedding-3-large"
    )
    
    pc = Pinecone(api_key=PINECONE_API_KEY)
    index = pc.Index(PINECONE_INDEX_NAME)
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    
    all_documents = []
    
    for pdf_path in pdf_files:
        print(f"üìÑ Processing: {pdf_path.name}")
        
        loader = PyPDFLoader(str(pdf_path))
        pages = loader.load()
        
        # Split into chunks
        chunks = text_splitter.split_documents(pages)
        all_documents.extend(chunks)
        
        print(f"   ‚úì Extracted {len(chunks)} chunks")
    
    print(f"\nüìù Indexing {len(all_documents)} total chunks...")
    
    # Create vector store
    vector_store = PineconeVectorStore(
        index=index,
        embedding=embeddings,
        text_key="text"
    )
    
    # Add documents in batches
    batch_size = 100
    for i in range(0, len(all_documents), batch_size):
        batch = all_documents[i:i+batch_size]
        vector_store.add_documents(batch)
        print(f"   ‚úì Indexed batch {i//batch_size + 1}/{(len(all_documents)-1)//batch_size + 1}")
    
    stats = index.describe_index_stats()
    print(f"\n‚úÖ Indexing complete!")
    print(f"üìä Total vectors in index: {stats['total_vector_count']}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--sample":
        # Index sample text
        index_sample_text()
    else:
        # Try to index PDFs, fall back to sample if none found
        data_dir = Path("data")
        if data_dir.exists() and list(data_dir.glob("*.pdf")):
            index_pdf_files()
        else:
            print("üìù No PDFs found. Indexing sample documents instead...")
            print("   (Use 'python index_documents.py' with PDFs in data/ folder)")
            print()
            index_sample_text()
